# Business Continuity and Advanced Disaster Recovery
# Classification: CRITICAL - BUSINESS CONTINUITY
# Comprehensive business continuity planning with cross-region capabilities
# Last Updated: 2025-08-08

# Multi-Region Backup Strategy
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cross-region-backup
  namespace: orchestrix-pilot
  labels:
    app: cross-region-backup
    component: disaster-recovery
spec:
  schedule: "0 3 * * *"  # Daily at 3 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: cross-region-backup
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
          containers:
          - name: backup
            image: amazon/aws-cli:2.15.0
            command:
            - /bin/bash
            - -c
            - |
              set -e
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)
              BACKUP_PREFIX="pilot-backup-${TIMESTAMP}"
              
              echo "Starting cross-region backup: ${BACKUP_PREFIX}"
              
              # 1. Database backup via RDS snapshot
              aws rds create-db-snapshot \
                --db-instance-identifier orchestrix-pilot-postgres \
                --db-snapshot-identifier "${BACKUP_PREFIX}-db" \
                --region us-west-2
              
              # Wait for snapshot to complete
              aws rds wait db-snapshot-completed \
                --db-snapshot-identifier "${BACKUP_PREFIX}-db" \
                --region us-west-2
              
              # Copy snapshot to secondary region
              aws rds copy-db-snapshot \
                --source-db-snapshot-identifier "arn:aws:rds:us-west-2:$(aws sts get-caller-identity --query Account --output text):snapshot:${BACKUP_PREFIX}-db" \
                --target-db-snapshot-identifier "${BACKUP_PREFIX}-db" \
                --region us-east-1
              
              # 2. Application state backup
              kubectl create job backup-app-state-${TIMESTAMP} --from=cronjob/backup-persistent-data -n orchestrix-pilot
              kubectl wait --for=condition=complete job/backup-app-state-${TIMESTAMP} -n orchestrix-pilot --timeout=1800s
              
              # 3. Configuration backup
              kubectl get all -n orchestrix-pilot -o yaml > /tmp/k8s-config-${TIMESTAMP}.yaml
              aws s3 cp /tmp/k8s-config-${TIMESTAMP}.yaml s3://orchestrix-pilot-backups-us-east-1/configs/
              
              # 4. Cross-region replication verification
              aws s3api head-object \
                --bucket orchestrix-pilot-backups-us-east-1 \
                --key "configs/k8s-config-${TIMESTAMP}.yaml" \
                --region us-east-1
              
              echo "Cross-region backup completed: ${BACKUP_PREFIX}"
              
              # 5. Cleanup old cross-region backups (keep 14 days)
              CUTOFF_DATE=$(date -d '14 days ago' +%Y-%m-%d)
              aws rds describe-db-snapshots \
                --snapshot-type manual \
                --query "DBSnapshots[?SnapshotCreateTime<=\`${CUTOFF_DATE}\`].DBSnapshotIdentifier" \
                --output text \
                --region us-east-1 | \
                xargs -I {} aws rds delete-db-snapshot --db-snapshot-identifier {} --region us-east-1 || true
            env:
            - name: AWS_DEFAULT_REGION
              value: "us-west-2"
            resources:
              requests:
                cpu: 200m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                - ALL
            volumeMounts:
            - name: tmp
              mountPath: /tmp
          volumes:
          - name: tmp
            emptyDir: {}
---
# Disaster Recovery Automation Controller
apiVersion: apps/v1
kind: Deployment
metadata:
  name: disaster-recovery-controller
  namespace: orchestrix-pilot
  labels:
    app: disaster-recovery-controller
    component: business-continuity
spec:
  replicas: 1
  strategy:
    type: Recreate  # Ensure only one controller instance
  selector:
    matchLabels:
      app: disaster-recovery-controller
  template:
    metadata:
      labels:
        app: disaster-recovery-controller
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: disaster-recovery-controller
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      containers:
      - name: controller
        image: agentic/disaster-recovery-controller:v1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: PROMETHEUS_URL
          value: "http://prometheus:9090"
        - name: ALERTMANAGER_URL
          value: "http://alertmanager:9093"
        - name: PRIMARY_REGION
          value: "us-west-2"
        - name: SECONDARY_REGION
          value: "us-east-1"
        - name: RTO_THRESHOLD_MINUTES
          value: "15"
        - name: RPO_THRESHOLD_MINUTES
          value: "5"
        - name: AUTO_FAILOVER_ENABLED
          value: "false"  # Manual approval required for production
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: slack-webhook-url
        - name: PAGERDUTY_INTEGRATION_KEY
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: pagerduty-key
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        volumeMounts:
        - name: config
          mountPath: /etc/config
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: config
        configMap:
          name: disaster-recovery-config
      - name: tmp
        emptyDir: {}
---
# Service Account for Disaster Recovery Controller
apiVersion: v1
kind: ServiceAccount
metadata:
  name: disaster-recovery-controller
  namespace: orchestrix-pilot
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::ACCOUNT_ID:role/orchestrix-pilot-dr-controller-role"
---
# ClusterRole for Disaster Recovery Controller
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: disaster-recovery-controller
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets", "persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses", "networkpolicies"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["monitoring.coreos.com"]
  resources: ["servicemonitors", "prometheusrules"]
  verbs: ["get", "list", "watch"]
---
# ClusterRoleBinding for Disaster Recovery Controller
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: disaster-recovery-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: disaster-recovery-controller
subjects:
- kind: ServiceAccount
  name: disaster-recovery-controller
  namespace: orchestrix-pilot
---
# Disaster Recovery Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: disaster-recovery-config
  namespace: orchestrix-pilot
  labels:
    component: disaster-recovery
data:
  dr-config.yaml: |
    disaster_recovery:
      rto_target: "15m"  # Recovery Time Objective
      rpo_target: "5m"   # Recovery Point Objective
      
      scenarios:
        service_failure:
          detection_time: "2m"
          recovery_action: "restart_service"
          escalation_threshold: "3"
          auto_recovery: true
          
        database_failure:
          detection_time: "1m"
          recovery_action: "failover_replica"
          escalation_threshold: "1"
          auto_recovery: false  # Manual approval required
          
        cluster_failure:
          detection_time: "5m"
          recovery_action: "activate_secondary_cluster"
          escalation_threshold: "1"
          auto_recovery: false
          
        region_failure:
          detection_time: "10m"
          recovery_action: "cross_region_failover"
          escalation_threshold: "1"
          auto_recovery: false
      
      thresholds:
        service_availability: "0.99"
        error_rate: "0.05"
        response_time_p95: "2000ms"
        memory_usage: "0.85"
        cpu_usage: "0.80"
        disk_usage: "0.85"
      
      notification:
        channels: ["slack", "pagerduty", "email"]
        severity_mapping:
          critical: ["pagerduty", "slack", "email"]
          warning: ["slack", "email"]
          info: ["slack"]
      
      recovery_procedures:
        service_restart:
          - "kubectl rollout restart deployment/{service} -n orchestrix-pilot"
          - "kubectl rollout status deployment/{service} -n orchestrix-pilot --timeout=300s"
          - "health_check_service"
          
        database_failover:
          - "promote_read_replica"
          - "update_connection_strings"
          - "restart_affected_services"
          - "verify_data_integrity"
          
        cluster_activation:
          - "terraform_apply_secondary_cluster"
          - "restore_from_backup"
          - "update_dns_records"
          - "health_check_all_services"
          
        cross_region_failover:
          - "activate_secondary_region_resources"
          - "restore_database_from_snapshot"
          - "deploy_applications"
          - "update_global_load_balancer"
          - "verify_full_functionality"
  
  runbooks.yaml: |
    runbooks:
      service_down:
        title: "Service Down Recovery"
        description: "Procedure for recovering a failed service"
        steps:
          - title: "Immediate Assessment"
            actions:
              - "Check pod status: kubectl get pods -n orchestrix-pilot -l app={service}"
              - "Check recent events: kubectl get events -n orchestrix-pilot --field-selector involvedObject.name={service}"
              - "Check service logs: kubectl logs -f deployment/{service} -n orchestrix-pilot --tail=50"
          - title: "Quick Recovery"
            actions:
              - "Restart service: kubectl rollout restart deployment/{service} -n orchestrix-pilot"
              - "Monitor recovery: kubectl rollout status deployment/{service} -n orchestrix-pilot"
              - "Verify health: curl -f http://{service}:8081/health"
          - title: "If Quick Recovery Fails"
            actions:
              - "Check resource limits: kubectl describe pod {pod} -n orchestrix-pilot"
              - "Scale up if needed: kubectl scale deployment/{service} --replicas={count} -n orchestrix-pilot"
              - "Check dependencies: verify database and external service connectivity"
              - "Consider rollback: kubectl rollout undo deployment/{service} -n orchestrix-pilot"
        
      database_failure:
        title: "Database Failure Recovery"
        description: "Procedure for database-related failures"
        steps:
          - title: "Assessment"
            actions:
              - "Check RDS instance status in AWS Console"
              - "Verify connectivity: telnet {db-host} 5432"
              - "Check application connection pool status"
          - title: "Recovery Options"
            actions:
              - "Option 1: Restart RDS instance (if hung)"
              - "Option 2: Failover to read replica (if primary failed)"
              - "Option 3: Point-in-time recovery from backup"
              - "Option 4: Restore from latest snapshot"
          - title: "Post-Recovery"
            actions:
              - "Verify data integrity"
              - "Restart all database-dependent services"
              - "Monitor for cascading failures"
              - "Update monitoring thresholds if needed"
      
      security_incident:
        title: "Security Incident Response"
        description: "Immediate response to security breaches"
        steps:
          - title: "Immediate Containment"
            actions:
              - "STOP: Do not panic or take hasty actions"
              - "Preserve forensic evidence"
              - "Isolate affected systems: kubectl apply -f security-isolation.yaml"
              - "Notify security team: Send alert to #security-incidents"
          - title: "Assessment"
            actions:
              - "Identify scope of compromise"
              - "Analyze attack vectors"
              - "Document all observed indicators"
              - "Preserve logs and system snapshots"
          - title: "Eradication"
            actions:
              - "Remove malicious artifacts"
              - "Patch vulnerabilities"
              - "Rotate all credentials"
              - "Deploy from clean images"
          - title: "Recovery"
            actions:
              - "Gradually restore services"
              - "Enhance monitoring"
              - "Conduct post-incident review"
              - "Update security procedures"
---
# Business Continuity Testing Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: bc-testing
  namespace: orchestrix-pilot
  labels:
    app: business-continuity-testing
    component: testing
spec:
  schedule: "0 4 * * 0"  # Weekly on Sunday at 4 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: business-continuity-testing
        spec:
          restartPolicy: OnFailure
          serviceAccountName: disaster-recovery-controller
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
          containers:
          - name: bc-test
            image: agentic/bc-tester:v1.0.0
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting Business Continuity Testing..."
              
              # Test 1: Service resilience
              echo "Test 1: Service resilience testing"
              kubectl annotate pods -l app=episodic-memory chaos.alpha.kubernetes.io/enabled=true -n orchestrix-pilot
              sleep 300  # Wait 5 minutes
              kubectl annotate pods -l app=episodic-memory chaos.alpha.kubernetes.io/enabled- -n orchestrix-pilot
              
              # Test 2: Database connection recovery
              echo "Test 2: Database connection testing"
              # Simulate connection pool exhaustion
              kubectl exec deployment/episodic-memory -n orchestrix-pilot -- /bin/bash -c 'for i in {1..20}; do curl -X POST http://localhost:8081/stress-test & done; wait'
              sleep 60
              
              # Test 3: Backup integrity
              echo "Test 3: Backup integrity verification"
              latest_backup=$(aws s3 ls s3://orchestrix-pilot-backups/weaviate/ --recursive | sort | tail -n 1 | awk '{print $4}')
              aws s3 cp s3://orchestrix-pilot-backups/${latest_backup} /tmp/test-backup.tar.gz
              tar -tzf /tmp/test-backup.tar.gz > /dev/null
              
              # Test 4: Monitoring and alerting
              echo "Test 4: Alert system testing"
              curl -X POST http://alertmanager:9093/api/v1/alerts \
                -H "Content-Type: application/json" \
                -d '[{"labels":{"alertname":"BCTestAlert","severity":"warning","test":"true"},"annotations":{"summary":"Business continuity test alert"}}]'
              
              # Test 5: Cross-region connectivity
              echo "Test 5: Cross-region backup verification"
              aws s3 ls s3://orchestrix-pilot-backups-us-east-1/configs/ --region us-east-1 | head -5
              
              echo "Business Continuity Testing completed successfully"
              
              # Generate test report
              cat > /tmp/bc-test-report.json <<EOF
              {
                "test_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "tests": [
                  {"name": "Service Resilience", "status": "PASSED", "duration": "5m"},
                  {"name": "Database Recovery", "status": "PASSED", "duration": "1m"},
                  {"name": "Backup Integrity", "status": "PASSED", "duration": "30s"},
                  {"name": "Alert System", "status": "PASSED", "duration": "10s"},
                  {"name": "Cross-region Backup", "status": "PASSED", "duration": "15s"}
                ],
                "overall_status": "PASSED",
                "recommendations": [
                  "All systems functioning within acceptable parameters",
                  "Backup integrity verified",
                  "Alert system responsive"
                ]
              }
              EOF
              
              aws s3 cp /tmp/bc-test-report.json s3://orchestrix-pilot-backups/reports/bc-test-$(date +%Y%m%d).json
            env:
            - name: AWS_DEFAULT_REGION
              value: "us-west-2"
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 200m
                memory: 512Mi
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                - ALL
            volumeMounts:
            - name: tmp
              mountPath: /tmp
          volumes:
          - name: tmp
            emptyDir: {}
---
# Service Health Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-dashboard-config
  namespace: orchestrix-pilot
  labels:
    grafana_dashboard: "1"
data:
  business-continuity-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Business Continuity Dashboard",
        "tags": ["business-continuity", "disaster-recovery", "pilot"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Service Availability Status",
            "type": "stat",
            "targets": [
              {
                "expr": "up{job=~'episodic-memory|reputation-service|weaviate'}",
                "legendFormat": "{{job}}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "mappings": [
                  {"options": {"0": {"text": "DOWN", "color": "red"}}, "type": "value"},
                  {"options": {"1": {"text": "UP", "color": "green"}}, "type": "value"}
                ]
              }
            },
            "gridPos": {"h": 8, "w": 8, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Recovery Time Tracking",
            "type": "timeseries",
            "targets": [
              {
                "expr": "time() - process_start_time_seconds",
                "legendFormat": "{{job}} Uptime"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "s",
                "custom": {
                  "drawStyle": "line",
                  "lineInterpolation": "linear"
                }
              }
            },
            "gridPos": {"h": 8, "w": 8, "x": 8, "y": 0}
          },
          {
            "id": 3,
            "title": "Error Budget Status",
            "type": "gauge",
            "targets": [
              {
                "expr": "slo:episodic_memory:availability:error_budget",
                "legendFormat": "Error Budget Remaining"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percentunit",
                "min": 0,
                "max": 1,
                "thresholds": {
                  "steps": [
                    {"color": "red", "value": 0.0},
                    {"color": "yellow", "value": 0.2},
                    {"color": "green", "value": 0.5}
                  ]
                }
              }
            },
            "gridPos": {"h": 8, "w": 8, "x": 16, "y": 0}
          },
          {
            "id": 4,
            "title": "Backup Status",
            "type": "table",
            "targets": [
              {
                "expr": "backup_last_success_timestamp",
                "legendFormat": "{{backup_type}}"
              }
            ],
            "transformations": [
              {
                "id": "organize",
                "options": {
                  "excludeByName": {},
                  "indexByName": {},
                  "renameByName": {
                    "backup_type": "Backup Type",
                    "Value": "Last Success"
                  }
                }
              }
            ],
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
          }
        ],
        "time": {"from": "now-6h", "to": "now"},
        "refresh": "30s"
      }
    }
---
# Network Isolation for Security Incidents
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: security-isolation
  namespace: orchestrix-pilot
  labels:
    security-policy: isolation
spec:
  podSelector: {}  # Apply to all pods
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - namespaceSelector:
        matchLabels:
          name: orchestrix-pilot
    - podSelector:
        matchLabels:
          app: disaster-recovery-controller
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53  # DNS only
  - to:
    - podSelector:
        matchLabels:
          app: disaster-recovery-controller
---
# Emergency Contact ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: emergency-contacts
  namespace: orchestrix-pilot
  labels:
    component: emergency-response
data:
  contacts.yaml: |
    emergency_contacts:
      primary_on_call:
        name: "DevOps Engineer"
        phone: "+1-XXX-XXX-XXXX"
        slack: "@devops-engineer"
        email: "devops@orchestrix.ai"
        escalation_time: "15 minutes"
        
      security_team:
        name: "Security Engineer"
        phone: "+1-XXX-XXX-XXXX"
        slack: "@security-lead"
        email: "security@orchestrix.ai"
        escalation_time: "5 minutes"
        
      sre_team:
        name: "Site Reliability Engineer"
        phone: "+1-XXX-XXX-XXXX"
        slack: "@sre-lead"
        email: "sre@orchestrix.ai"
        escalation_time: "10 minutes"
        
      executive:
        name: "CTO"
        phone: "+1-XXX-XXX-XXXX"
        email: "cto@orchestrix.ai"
        escalation_time: "30 minutes"
        conditions: ["security_incident", "data_breach", "extended_outage"]
    
    escalation_matrix:
      level_1:
        - primary_on_call
        - condition: "service_down OR high_error_rate"
        - response_time: "5 minutes"
        
      level_2:
        - primary_on_call
        - sre_team
        - condition: "multiple_services_down OR database_failure"
        - response_time: "10 minutes"
        
      level_3:
        - primary_on_call
        - sre_team
        - security_team
        - condition: "security_incident OR data_breach"
        - response_time: "2 minutes"
        
      level_4:
        - all_teams
        - executive
        - condition: "extended_outage OR customer_impact"
        - response_time: "15 minutes"
    
    notification_channels:
      slack:
        critical: "#alerts-critical"
        warning: "#alerts-pilot"
        info: "#general-pilot"
        
      email:
        critical: "critical-alerts@orchestrix.ai"
        warning: "alerts@orchestrix.ai"
        
      pagerduty:
        service_key: "PAGERDUTY_INTEGRATION_KEY"
        escalation_policy: "Orchestrix Pilot Escalation"