# **From Monolithic Procedures to a Generative Skill Nexus: Architecting the Next Generation of Procedural Memory**

## **Part I: The Imperative for Compositionality: Deconstructing Procedural Knowledge**

The evolution of artificial intelligence from narrow problem-solvers to generalist agents hinges on the development of sophisticated, human-like cognitive architectures. Central to this evolution is the concept of memory. An agent's ability to learn, adapt, and generalize is fundamentally constrained by how it stores, recalls, and utilizes past experience. The current implementation of Procedural Memory within the system, while functional, represents a significant bottleneck. It stores successful action sequences as monolithic, rigid scripts. This approach, akin to a simple macro library, is powerful for re-executing known solutions in identical contexts but is critically brittle; it fails when faced with novel tasks that are similar, but not identical, to past experiences.

This report presents a comprehensive research initiative to transform this static repository into a flexible and generative skill library. The objective is to deconstruct monolithic procedures into fundamental, reusable "skills" and to develop a framework for dynamically composing these skills to solve novel problems. This paradigm shift moves the agent from rote memorization to a more human-like form of skill acquisition and application, dramatically enhancing its adaptability and problem-solving prowess in complex, open-ended environments.

### **Section 1.1: Foundational Concepts: Re-evaluating Memory in Intelligent Agents**

To build more adaptable agents, it is imperative to first ground their cognitive architecture in a more nuanced understanding of memory, drawing parallels from human cognition. Human memory is not a singular faculty but a layered, multifaceted system, a model that advanced AI systems are increasingly designed to emulate.1 The primary forms of long-term memory relevant to this discussion are procedural, semantic, and episodic memory, each playing a distinct yet interconnected role in intelligent behavior.2

* **Procedural Memory:** This is the "how-to" knowledge, often implicit and acquired through repetition and practice.4 It governs the execution of skills and tasks, from a robot operating factory machinery to an AI agent executing a multi-step troubleshooting sequence.1 In AI, this is typically the domain of reinforcement learning (RL) and imitation learning, where an agent learns to perform actions to achieve goals.1  
* **Semantic Memory:** This constitutes the agent's repository of structured, factual knowledge—the "knowing-that".2 It includes facts, concepts, and rules about the world, often stored in knowledge bases, knowledge graphs, or the weights of large language models (LLMs) via vector embeddings.2 An AI legal assistant retrieving case precedents relies on its semantic memory.2  
* **Episodic Memory:** This is the memory of specific, autobiographical events and experiences, capturing the "who, what, when, and where" of past interactions.1 For an AI agent, this could involve logging key events, actions, and their outcomes, allowing it to learn from and contextualize personal interactions.1

The historical SOAR architecture provided an early, influential model by separating immediate working memory from a long-term procedural memory composed of production rules, anticipating the modern need for systems that can retain and refine knowledge over time.7

The Brittleness of Monolithic Procedures  
The current system's reliance on storing complete, successful action sequences as monolithic units is a primary source of its inflexibility. This approach is analogous to a person memorizing a single, fixed route to a destination without understanding the underlying map or the principles of navigation. Such a system is effective only as long as the exact path is available and the goal remains unchanged. It breaks down when faced with the slightest deviation, such as a road closure or a change in destination. This brittleness renders the agent ineffective in "wicked" learning environments, where rules may shift, feedback is ambiguous, and novelty is the norm.8 A policy trained on one specific set of tools or environmental conditions requires expensive and time-consuming retraining to handle even minor variations, a critical limitation for robust, real-world deployment.9  
The Paradigm Shift: From Macros to Skills  
To overcome this brittleness, a fundamental paradigm shift is required: moving from storing rigid procedures to learning a library of composable "skills." This represents a transition from encoding "what to do" in a specific situation to understanding "how to do" a particular action that can be applied flexibly across many situations.4 Instead of a single, long script for "make coffee," the agent would possess a library of skills like  
grasp\_mug, operate\_grinder, pour\_water, and press\_button. This decomposition transforms the procedural memory from a static macro library into a generative skill library, providing the building blocks for constructing solutions to a vast array of novel problems. This generative capability is the cornerstone of creating truly adaptive agents that can thrive in the dynamic, open-ended, and ever-changing environments characteristic of real-world applications.10

This architectural evolution, however, cannot be realized by focusing on procedural memory in isolation. The research strongly indicates that the efficacy of a sophisticated skill library is contingent upon its deep integration with the agent's other memory systems. The very process of decomposing complex tasks into semantically meaningful skills, as will be discussed, often relies on the vast world knowledge stored in the semantic memory of LLMs. Likewise, the process of selecting the correct skill to apply in a novel situation necessitates a rich contextual understanding of the current state and past events, a function of episodic memory. A compelling case study is TechSee's Sophie AI, which synthesizes semantic, episodic, and procedural memory to deliver contextually appropriate and efficient problem resolution.1 Therefore, the enhancement of procedural memory is not merely an algorithmic challenge but a systemic architectural one. The

MemoryManager must be reconceptualized not as a silo for procedures, but as a central hub that orchestrates a continuous dialogue between memory types. This integration is the prerequisite for enabling the flexible skill composition and application that this research initiative targets.

### **Section 1.2: Unsupervised Skill Discovery: Learning the Atomic Units of Behavior**

Before an agent can compose skills, it must first acquire them. The most powerful and general approach to this is through Unsupervised Reinforcement Learning (URL), also known as skill discovery. URL enables an agent to autonomously build a rich behavioral repertoire by interacting with its environment without any external, task-specific rewards.12 This "pre-training" phase is foundational to our goal, as it allows the agent to learn the atomic units of behavior—the fundamental skills—that will populate its generative library. These skills can later be rapidly recombined to solve a multitude of downstream tasks.13

The Mutual Information (MI) Paradigm  
A dominant paradigm in URL is the maximization of mutual information (MI) between the agent's state S and a latent skill variable Z, typically sampled from a simple prior distribution like a uniform categorical distribution.12 The objective is formally expressed as maximizing  
I(S;Z). The intuition is straightforward: if the mutual information is high, then observing the state s that the agent reaches provides a great deal of information about the skill z that was chosen. This encourages the agent to learn a set of skills that are behaviorally distinct, each leading to different, predictable regions of the state space.15

In practice, directly optimizing I(S;Z)=H(Z)−H(Z∣S) is intractable. Most methods instead optimize a variational lower bound, which involves training a discriminator network qϕ​(z∣s) to predict the skill z given a state s. The intrinsic reward for the agent is then derived from the log-likelihood of this discriminator, r(s,z)=logqϕ​(z∣s).15

However, this foundational approach has a significant limitation: it often produces "entangled" skills.12 An entangled skill is one where a single latent variable

z simultaneously influences multiple, independent aspects of the environment. For example, a single skill might simultaneously change a car's speed, steering, and headlights, making it extremely difficult for a higher-level policy to learn how to turn on the lights without also altering the car's trajectory.12 Furthermore, simple MI-based methods can exhibit a "staticness bias," where they learn skills that simply reach and occupy different static regions of the state space rather than performing functionally rich, dynamic behaviors.13

Key Innovation Spotlight: Disentangled Unsupervised Skill Discovery (DUSDi)  
The Disentangled Unsupervised Skill Discovery (DUSDi) method was developed specifically to address the critical problem of skill entanglement.12 DUSDi's core innovation is to leverage environments where the state space can be naturally factored into independent components, such as the individual states of multiple objects in a robotic manipulation task or the different controllable aspects of a vehicle.

* **Core Idea:** DUSDi decomposes the latent skill variable z into a set of components, z=(z1​,...,zN​), corresponding to the N factors of the state space, S=(S1​,...,SN​). The learning objective is then structured to encourage each skill component zi​ to exclusively influence its corresponding state factor Si​, while actively discouraging it from affecting other state factors Sj​ where j=i.12  
* Objective Function: This is achieved by designing a novel intrinsic reward function based on the MI principle. For each skill component zi​, the agent is rewarded for maximizing the mutual information with its target state factor, I(Si​;zi​), and penalized for having high mutual information with any other state factor, I(Sj​;zi​) for j=i. Using the same variational approximation as before, this leads to an intrinsic reward function for the entire skill vector z that can be expressed as:  
  r(s,z)=i=1∑N​​logqϕi​​(zi​∣Si​)−j=i∑​logqϕj​​(zi​∣Sj​)​

  This objective explicitly rewards disentanglement by promoting high correlation between a skill component and its designated state factor while penalizing cross-factor correlations.16  
* **Optimization via Value Factorization:** A naive implementation would require learning a monolithic Q-function over the joint skill space, which can be complex. DUSDi introduces a crucial optimization: value factorization. It learns a separate Q-function, Qi​(s,a,zi​), for each skill component. The overall Q-function for the composite skill z is then simply the sum of the individual component Q-functions: Q(s,a,z)=∑i=1N​Qi​(s,a,zi​). This decomposition aligns perfectly with the disentangled objective, simplifying the learning problem and improving stability.12

Alternative and Complementary Approaches  
While DUSDi provides a powerful solution to entanglement, other research avenues also seek to improve upon basic MI-based skill discovery.

* **Wasserstein Distance:** The KL-divergence underlying MI is not a true distance metric. This has led to the development of frameworks like Wasserstein Unsupervised Reinforcement Learning (WURL), which use the Wasserstein distance to measure the geometric distance between the state distributions induced by different skills.18 This approach can discover a more diverse set of "vertex" skills that lie at the boundaries of the space of achievable behaviors, potentially offering a richer skill set for downstream tasks.19  
* **Contrastive Learning and State-Distribution Constraints:** Other methods use techniques from self-supervised visual learning. Contrastive learning frameworks learn to pull together representations of trajectories from the same skill while pushing apart those from different skills.13 Methods like Constrained Ensemble Skill Discovery (CeSD) explicitly partition the state space and assign skills to explore distinct regions, using state-distribution constraints to enforce both diversity and high state coverage.13

The effectiveness of these skill discovery methods, particularly a sophisticated approach like DUSDi, reveals a deeper principle about the relationship between the agent and its environment. The success of DUSDi is predicated on the existence of a factorizable state space, which serves as a powerful inductive bias for the learning algorithm.12 This is not merely a technical convenience; it reflects a fundamental property of many real-world environments where the dynamics exhibit sparsity and modularity. For instance, a robot's arm movement is largely independent of a separate conveyor belt's state.12 If the environment were a fully entangled, chaotic system where every variable causally influenced every other, learning disentangled skills would be impossible. This leads to a critical architectural consideration: the process of building a generative skill library is not solely the responsibility of the learning algorithm but is deeply intertwined with the agent's perception system. To maximize the effectiveness of this research, the system's state representation, as defined in

BLUEPRINT.md, should be engineered to be as factored and object-centric as possible. This creates a powerful feedback loop: a factored state representation enables the discovery of disentangled skills, which are inherently more composable, thus simplifying the solution of complex downstream tasks.

### **Section 1.3: LLM-Guided Decomposition: Injecting Semantic Reasoning into Skill Discovery**

While unsupervised methods excel at discovering a diverse set of behaviors, these discovered skills often lack semantic meaning or direct relevance to human-centric tasks. They answer the question, "What *can* the agent do?" but not "What *should* the agent learn to do?".22 Large Language Models (LLMs) have emerged as a transformative technology to bridge this semantic gap, offering a powerful mechanism for translating high-level, human-understandable goals into the structured, machine-executable sub-tasks that form a meaningful skill library.23

Frameworks for LLM-Guided Skill Learning  
Several innovative frameworks have been proposed to integrate the reasoning capabilities of LLMs with the learning mechanisms of RL.

* **L2S (Language2Skills):** This framework provides a direct and practical approach to automated skill creation. Given a natural language description of a complex task (e.g., "stack the red block on the blue block"), an LLM is prompted to decompose it into a sequence of named sub-tasks. Critically, the LLM's role extends beyond mere decomposition; it also generates the *dense reward function* and the *termination condition* for each sub-task, typically as executable Python code. This code is then used by a standard RL algorithm to train a policy for that specific, semantically-grounded skill. This process automates the arduous and often unintuitive task of reward engineering, a major bottleneck in traditional RL.23  
* **LDSC (LLM-guided Semantic Deep Skill Chaining):** This approach embeds the LLM at the apex of a three-level hierarchy. For a given high-level task, the LLM acts as a planner, generating a sequence of *semantic subgoals*. These abstract subgoals are then passed to a mid-level "option policy" (the HRL Manager, discussed in Part II), which selects the appropriate learned skill to achieve the current subgoal. Finally, a low-level action policy executes the primitive motor commands. LDSC leverages the LLM for high-level, logical reasoning and long-horizon planning, structuring the entire decision-making process.24  
* **LGSD (Language Guided Skill Discovery):** This framework takes a different tack, using the LLM to directly shape the skill discovery process itself. Instead of maximizing a purely information-theoretic objective, LGSD uses the LLM to define a *semantic distance metric* between skills. The intrinsic reward encourages the agent to learn a set of skills that are maximally diverse in a semantic sense, as judged by the LLM. This allows a user to provide a simple prompt like "learn different ways to move," and the agent discovers skills that are semantically distinct (e.g., walking, hopping, crawling).22

Benefits and Drawbacks  
The integration of LLMs offers profound benefits. It provides a natural mechanism for structured exploration, as the agent is guided to learn specific, meaningful sub-tasks rather than exploring randomly.24 This greatly accelerates the learning process and enhances the generalization of learned policies, as they are grounded in semantic concepts that can transfer across tasks.24 Perhaps most powerfully, it enables zero-shot control and composition of skills via natural language commands, a significant step towards more intuitive human-agent interaction.22  
However, this power comes with significant caveats. These methods are fundamentally dependent on the quality and reliability of the LLM's output. An LLM might generate incorrect, inefficient, or unsafe sub-task decompositions or reward functions, making the overall system brittle.23 This dependency often necessitates carefully engineered few-shot prompts and may require supplementary verification or evolutionary search mechanisms to filter or refine the LLM's suggestions. Furthermore, there is a persistent risk of "data contamination," where the LLM appears to solve a problem by leveraging knowledge memorized from its vast training data rather than engaging in genuine, context-aware reasoning about the specific task at hand.27

The emergence of these LLM-guided frameworks signifies a crucial evolution in the philosophy of skill discovery. Traditional URL is a fundamentally bottom-up process; it discovers the space of all possible behaviors based on an intrinsic, task-agnostic objective like maximizing mutual information. In contrast, LLM-guided approaches introduce a top-down, goal-directed pressure on the learning process. The LLM, given a high-level task description, acts as an automated curriculum designer, defining the very skills that are worth learning.28 It constrains the agent's vast search space of potential behaviors to a smaller, semantically meaningful, and task-relevant subset. This is a paradigm shift from undirected exploration to guided, purposeful skill acquisition. This has a direct architectural implication for our system: the

MemoryManager should be designed to store not just the raw skill policy (the network weights), but also the rich semantic scaffold provided by the LLM. This includes the natural language description of the skill, the generated subgoal sequence, and even the code for the reward and termination functions. Storing this metadata would create a far more interpretable, debuggable, and extensible skill library, aligning perfectly with the goal of creating a truly generative and flexible procedural memory.

## **Part II: The Art of Composition: Assembling Skills into Novel Procedures**

Once a library of fundamental, reusable skills has been discovered and stored, the next critical challenge is to enable the agent to intelligently select and sequence these skills to solve novel, complex problems. This is the art of composition. The primary and most powerful computational framework for achieving this is Hierarchical Reinforcement Learning (HRL). HRL provides the structural backbone necessary to move beyond executing individual skills and begin orchestrating them into coherent, goal-directed procedures.

### **Section 2.1: Hierarchical Reinforcement Learning (HRL) as the Compositional Backbone**

Standard, or "flat," reinforcement learning suffers from a severe scaling problem, often called the "curse of dimensionality".29 As the complexity of a task grows, the state-action space explodes, making it computationally infeasible for an agent to learn an optimal policy through trial and error. HRL directly addresses this challenge by introducing multiple levels of policies that operate at different levels of temporal abstraction.29

**Core Principles of HRL**

* **Temporal Abstraction:** The core idea of HRL is to break down decision-making across different timescales. A high-level policy, or "Manager," makes abstract, long-term decisions (e.g., "go to the kitchen," "pick up the cup"). A low-level policy, or "Worker," is then responsible for executing the sequence of primitive, short-term actions required to achieve that sub-goal.30 This decomposition allows the high-level policy to plan over long horizons without getting bogged down in low-level motor control details.  
* **Structured Exploration and Credit Assignment:** HRL fundamentally changes how an agent explores its environment. Instead of exploring via random primitive actions, the high-level policy explores in the much smaller and more meaningful space of sub-policies or skills. This leads to more structured and efficient exploration.30 Similarly, it simplifies the problem of credit assignment. If a task fails, it is easier to identify which high-level sub-goal was poor, rather than trying to assign blame to one of thousands of primitive actions in a long sequence.29

Key Training Paradigms  
The interaction between the high-level and low-level policies can be structured in two primary ways:

* **Bottom-Up Training:** In this approach, the low-level skills are discovered and trained first, often using the unsupervised methods described in Part I. Once these skills are learned and their policies are "frozen," a high-level composition policy is trained on top of them, learning how to sequence these fixed skills to solve various tasks. This is the most straightforward method, as the high-level policy learns over a stable set of actions (the skills). However, it can be sample-inefficient, as the initial discovery phase may learn many skills that are not actually useful for the tasks the agent will ultimately face.33  
* **Top-Down Training:** Here, the learning process starts with the overall goal. The high-level policy proposes a subgoal that it believes will help achieve the final goal, and the low-level policy is then trained specifically to reach that proposed subgoal. This approach is generally more efficient for solving a single, complex task because it focuses the learning process only on relevant skills. Its primary challenge, however, is the non-stationarity of the learning environment for the high-level policy. The high-level policy is trying to learn a mapping from states to subgoals, but the low-level policy's ability to achieve those subgoals is constantly changing as it learns. This can destabilize the entire training process.33

The Non-Stationarity Challenge  
This issue of non-stationarity is one of the most significant hurdles in HRL.34 The high-level policy is essentially trying to hit a moving target. If the low-level policy is not yet competent, the high-level policy might receive negative feedback for choosing a perfectly valid subgoal, simply because the low-level policy failed to execute it. This can lead the high-level policy to incorrectly learn that the subgoal is "bad" and avoid it in the future. Modern HRL frameworks like Hierarchical Actor-Critic (HAC) and HIRO have developed sophisticated techniques, such as using hindsight relabeling for the high-level policy's transitions, to mitigate this instability and allow for more effective end-to-end training.36  
The choice between these two training paradigms is not merely a technical one; it reflects a fundamental trade-off between skill reusability and task-specific efficiency. A bottom-up approach, particularly one that leverages unsupervised skill discovery, is inherently geared towards creating a general-purpose, task-agnostic skill library. The resulting skills are designed to be broadly applicable and reusable. A top-down approach, conversely, is optimized for solving a specific, known complex task with maximal efficiency, generating subgoals tailored precisely to that problem. The user query for this research specifies the goal of a "flexible and generative skill library," which strongly favors reusability and adaptability to a wide range of *novel* tasks. This suggests that a purely top-down approach would be too narrow. Therefore, an optimal architecture for the BLUEPRINT.md system would likely be a hybrid one. The initial SkillLibrary should be populated using a bottom-up, unsupervised discovery method (like DUSDi) to ensure a foundation of general, reusable, and disentangled skills. Subsequently, a top-down HRL controller can be deployed to dynamically compose these pre-existing skills to solve specific tasks. This top-down layer could also be empowered to fine-tune existing skills or even learn new, highly specialized skills if the foundational library proves insufficient for a particularly novel challenge, thus combining the generality of bottom-up discovery with the efficiency of top-down execution.

### **Section 2.2: A Comparative Analysis of HRL Frameworks for Skill Composition**

Within the broader HRL paradigm, several distinct frameworks have emerged, each representing a different philosophy on how to represent, learn, and compose skills. The three most influential are the Options framework, Feudal Networks, and Goal-Conditioned Policies. Understanding their respective strengths and weaknesses is crucial for making an informed architectural decision.

**The Options Framework (Sutton et al.)**

* **Representation:** The fundamental unit of temporal abstraction is the "option." An option o is formally defined as a tuple ⟨I,β,π⟩, where I is the initiation set (the states where the option can be started), β is the termination condition (a function giving the probability of the option terminating in a given state), and π is the intra-option policy that is followed while the option is active.30 Options can be seen as temporally extended macro-actions that the agent can choose in addition to its primitive actions.  
* **Learning and Composition:** A high-level policy learns a policy-over-options, selecting which option to execute from a given state. The central and most difficult challenge within this framework is "option discovery"—the problem of automatically identifying useful options (i.e., defining their initiation sets and termination conditions) from data.29 Early approaches relied on heuristics, such as identifying "bottleneck" states in the environment's state-space graph and creating options to travel between them.40 More recent methods, like the Option-Critic architecture, attempt to learn all components of the options, including the termination conditions and intra-option policies, in an end-to-end fashion using policy gradients.42  
* **Critique:** While elegant, the options framework faces practical challenges. End-to-end learning of options is notoriously unstable and prone to degenerating into trivial solutions. For example, the agent might learn a single option that solves the entire task, or it might learn options that terminate after every single timestep, effectively collapsing the hierarchy back into a flat policy. These issues often require complex regularization schemes to encourage the discovery of meaningful, temporally-extended options.42

**Feudal Networks (FuN) (Dayan, Hinton, Vezhnevets et al.)**

* **Representation:** Inspired by the concept of feudalism, this framework proposes a strict Manager/Worker hierarchy.44 The Manager operates at a low temporal resolution (e.g., making a decision every  
  c timesteps) and sets abstract *goals* for the Worker. These goals are not explicit states but are typically represented as vectors in a latent space learned by the Manager itself.42 The Worker operates at every timestep, taking the Manager's current goal as an additional input and executing primitive actions to achieve it.  
* **Learning and Composition:** A key principle of Feudal RL is the decoupling of learning signals, or "reward hiding".44 The Manager is trained via standard RL (e.g., with an approximate transition policy gradient) to maximize the  
  *extrinsic* reward from the environment. It learns to set goals that, if achieved, will lead to high environmental reward. The Worker, in contrast, is shielded from the extrinsic reward. It is trained to maximize an *intrinsic* reward, which is based on its progress towards the goal set by the Manager (e.g., maximizing the cosine similarity between the change in state representation and the goal vector).42  
* **Critique:** This decoupled, goal-based architecture is a major strength. It encourages the emergence of semantically meaningful and reusable sub-policies, as the Manager learns to set consistent goals for recurring sub-problems (e.g., "go up the ladder"). The use of directional goals in a latent space, rather than absolute state goals, provides a powerful form of generalization, as the same directional goal can be useful in many different parts of the state space.42

**Goal-Conditioned Policies (GCRL)**

* **Representation:** This is a more general and flexible framework that has become the de facto standard for the low-level controller in many modern HRL systems.36 Instead of learning a separate policy for each skill or option, GCRL involves training a single, universal policy,  
  π(a∣s,g), that is explicitly conditioned on a goal g. This single policy can, in principle, learn to reach an entire continuous space of goals.  
* **Learning and Composition:** GCRL policies are often trained using Hindsight Experience Replay (HER). HER is a powerful technique that addresses the challenge of sparse rewards by treating every failed trajectory as a successful one for a different goal. After an episode, the agent relabels the transitions in its replay buffer with the goal it *actually* achieved, rather than the one it was trying to reach. This creates a dense and highly effective learning signal, dramatically improving sample efficiency.48 In an HRL context, a high-level policy is trained to output a sequence of goals, which are then fed into the universal goal-conditioned low-level policy.  
* **Critique:** The primary strength of GCRL is its sample efficiency and flexibility. A single policy that can generalize across a continuous goal space is more powerful and scalable than maintaining a discrete set of options. It is often preferred in practice because it requires no prior knowledge about the task and demonstrates significant performance gains on benchmark tasks.46

The following table provides a side-by-side comparison to distill these differences into an actionable decision-making tool.

| Feature | The Options Framework | Feudal Networks (FuN) | Goal-Conditioned Policies (GCRL) |
| :---- | :---- | :---- | :---- |
| **Skill Representation** | A discrete set of "options," each a tuple of ⟨I, β, π⟩. Temporally extended macro-actions. 30 | A Manager/Worker hierarchy. The Manager sets abstract, directional goals in a learned latent space. 42 | A single, universal policy $\\pi(a |
| **Learning Mechanism** | High-level: Learns a policy over options. Low-level: Intra-option policies learned via RL, often with predefined pseudo-rewards or end-to-end with difficulty. 29 | Decoupled learning. Manager trained on extrinsic reward via policy gradients. Worker trained on intrinsic reward (goal achievement). 42 | Typically trained with off-policy RL (e.g., DDPG, SAC) augmented with Hindsight Experience Replay (HER) for dense rewards. 43 |
| **Goal Specification** | Goals are implicit in the option's definition (e.g., reaching a bottleneck state or a termination condition). 40 | Goals are abstract vectors in a latent space, representing directions of change, generated by the Manager. 42 | Goals are explicit states g ∈ S or points in a goal space, provided by a higher-level policy or the environment. 46 |
| **Key Strengths** | Provides a formal, theoretically-grounded model for temporal abstraction. 29 | Decoupled structure leads to robust, emergent, and semantically meaningful sub-policies. Strong generalization. 42 | Highly sample-efficient in sparse reward tasks. A single policy can generalize across a continuous space of goals. 48 |
| **Critical Weaknesses** | Option discovery is difficult. End-to-end learning is unstable and prone to degeneration without careful regularization. 42 | Complex architecture. Training can be sensitive. Less theoretical analysis compared to the options framework. 42 | Can be biased towards optimizing for closer goals. Performance depends heavily on the quality of the goal representation. 48 |
| **Primary Use Case** | Problems with clear sub-task structures or "bottlenecks" that can be defined a priori or discovered. 40 | Tasks requiring long-horizon credit assignment and the emergence of a reusable, semantic skill hierarchy. 42 | The dominant choice for the low-level controller in modern HRL systems, especially in robotics and sparse-reward environments. 36 |

### **Section 2.3: Advanced Compositional Techniques: Skill Chaining and Planning**

Beyond the core HRL frameworks, more advanced techniques exist for structuring the composition of skills, enabling agents to tackle exceptionally long-horizon tasks. These methods often combine the learned, low-level control of HRL with higher-level, more structured reasoning processes.

Skill Chaining  
Skill chaining is a clever and intuitive method for autonomously constructing sequences of skills that lead to a distant goal.54 The process works backward from the goal:

1. First, the agent learns a skill (an option) whose goal is to reach the final, rewarding state of the task. This skill will have an initiation set—the region of the state space from which it can be reliably executed.  
2. The method then treats reaching this initiation set as a new, intermediate goal. It creates a *second* skill whose objective is to reach a state where the *first* skill can be initiated.  
3. This process is repeated recursively, creating a "chain" of skills, where each skill's purpose is to set up the conditions for the next skill in the chain to be executed.38

This effectively builds a path of overlapping initiation sets that leads the agent from a potentially very distant starting state all the way to the final goal. The Deep Skill Chaining (DSC) algorithm extends this concept to high-dimensional, continuous state spaces, making it applicable to complex robotics tasks.24

This process of skill chaining is more than just a composition technique; it is a powerful, self-generating form of curriculum learning. The field of curriculum learning focuses on improving an agent's performance by presenting it with tasks of gradually increasing difficulty.28 Skill chaining automates this process in a principled way. The first skill it learns—reaching the goal from a nearby state—is the easiest version of the task. The second skill—reaching the initiation set of the first skill—is a slightly harder task. Each link added to the chain represents a progressively more difficult reachability problem. In this way, skill chaining bootstraps the agent's capabilities from the goal outward, providing a natural curriculum that can be essential for solving problems with extremely sparse rewards and long temporal horizons, where random exploration would almost certainly fail.

Integrating HRL with Symbolic Planning  
A second powerful approach is to create a hybrid system that combines the strengths of HRL with classical symbolic AI planning. Symbolic planners excel at long-horizon, logical reasoning and can efficiently find a sequence of high-level, abstract actions to solve a problem. However, they lack the ability to ground these abstract actions in the continuous, noisy, and unpredictable real world. HRL, conversely, excels at learning robust, low-level control policies but struggles with long-term planning.  
A hybrid architecture leverages the best of both worlds.39 A high-level symbolic planner is used to generate a task plan, which consists of a sequence of abstract operators like

pick\_up(block\_A) or move\_to(door).58 This plan is then passed to an HRL execution module. The HRL system's

SkillLibrary contains learned policies corresponding to each of these symbolic operators. The high-level policy of the HRL system then simply executes the plan, invoking the correct low-level skill for each step.59 This approach has proven particularly effective in robotics, where complex manipulation tasks can be decomposed into a known sequence of symbolic steps, but each step requires a complex, learned policy to be executed reliably in the physical world.39

## **Part III: Ensuring Robustness: Generalization and Lifelong Learning**

Creating a library of composable skills is a significant step, but its true value is realized only if the skills are robust, generalizable, and can be expanded upon over time without degrading past performance. This part of the report addresses the critical challenges of generalization—applying learned knowledge to new situations—and lifelong learning—continuously acquiring new knowledge over an extended period.

### **Section 3.1: The Generalization Challenge in Reinforcement Learning**

Generalization in reinforcement learning is a fundamentally harder problem than in supervised learning, and this distinction is crucial for setting realistic expectations and designing appropriate solutions.10

RL Generalization vs. Supervised Learning  
In a typical supervised learning task like image classification, the goal is to train a model that performs well on average across a distribution of unseen data. The model is successful if its expected error on the test set is low. In contrast, RL generalization often has a more stringent requirement: instance-optimality. When an RL agent is deployed in a new, unseen test environment, it is expected to perform near-optimally for that specific instance.10 This is a much higher bar. Without strong assumptions about the relationship between training and testing environments, achieving instance-optimality can be statistically intractable. Theoretical analyses have shown that it is often impractical to directly learn a policy during a training phase that is guaranteed to be near-optimal for any specific test environment it might encounter.10  
Formalizing Generalization: Contextual MDPs (CMDPs)  
To study generalization rigorously, the field has adopted the formalism of the Contextual Markov Decision Process (CMDP).60 A CMDP represents not a single environment, but a family or distribution of environments (MDPs) that share a common structure (e.g., state and action spaces) but differ in some underlying "context," such as their dynamics  
P, reward functions R, or initial state distributions μ. The generalization problem is then framed as training an agent on a set of contexts, M\_train, and evaluating its performance on a different, unseen set of test contexts, M\_test. The ability to perform well on M\_test without any further training is known as zero-shot policy transfer.61

The Role of Exploration and Representation  
Improving generalization in RL is not merely a matter of learning a better policy function. Because the agent collects its own data, the exploration strategy during training plays a pivotal role.60 Trajectories that are suboptimal for achieving a goal in one training environment might contain invaluable information for finding the optimal policy in a different test environment. For example, exploring how to navigate around an obstacle on the left in one training level might be the key to solving a test level where the goal is behind a similar obstacle. This means the data collection process itself must be designed with generalization in mind, for instance by training on a wide and diverse distribution of environments.62  
The quality of the agent's learned representations is also paramount. A robust policy must learn to rely on features of the environment that are invariant across contexts, while ignoring spurious correlations specific to the training set. This is where learning disentangled representations, as discussed in the context of DUSDi, becomes critical for generalization. By isolating the causal factors of the environment, the agent can learn skills that are more likely to transfer correctly to new situations.63

A particularly relevant and challenging form of generalization is the ability to adapt to new *actions* or skills. Imagine a robot that has learned to use a hammer and a screwdriver, and is suddenly presented with a wrench. A truly general agent should be able to infer the properties and affordances of the new tool—perhaps through a brief period of interaction or observation—and incorporate this new skill into its decision-making process without requiring complete retraining from scratch.9 This is a direct analogue to the goal of our

MemoryManager: to create a skill library that is not fixed, but can be dynamically extended with new, useful capabilities.

### **Section 3.2: From Single-Task Competence to Lifelong Skill Acquisition**

The ultimate vision for an intelligent agent is one that learns continuously throughout its existence, much like a human. This is the domain of Lifelong Reinforcement Learning (LRL), a paradigm where an agent faces a continuous stream of tasks. The two central goals of LRL are to enable positive forward transfer (using past knowledge to learn new tasks faster) and to prevent catastrophic forgetting (avoiding the unlearning of old skills when acquiring new ones).64

The Problem with Standard Transfer  
Simple approaches to preventing forgetting, which are often adapted from supervised learning, tend to be insufficient for the complexities of RL. For example, Elastic Weight Consolidation (EWC), which constrains updates to parameters deemed important for old tasks, can be too rigid. If a new task is fundamentally different from old ones, these constraints can severely hinder the agent's ability to adapt and find an optimal policy for the new task.64 Similarly, naively replaying data from old tasks alongside new task data can cause interference, forcing the agent to balance conflicting objectives and leading to suboptimal performance on all tasks.64 The core issue is that these methods often fail to effectively model the relationships between tasks.  
Architectures for Lifelong Reinforcement Learning  
More sophisticated architectures have been developed to specifically address the challenges of LRL.

* **Parameter Isolation and Masking:** These methods work by dedicating distinct parts of the neural network to different tasks, thereby protecting learned knowledge from being overwritten. A promising approach involves using a fixed, shared "backbone" network and learning task-specific "modulating masks." When a new task is introduced, the agent learns a new mask that is applied to the backbone. The backbone parameters remain unchanged, preserving all previously learned knowledge, while the new mask allows for learning the new task. This has been shown to be a superior approach for LRL in both discrete and continuous domains.68  
* **Rehearsal and Consolidation:** This class of methods explicitly retains knowledge of old tasks. Task rehearsal, for instance, involves saving the policy learned for a previous task. When a new task is being learned, this saved policy is used to generate "virtual" training examples from the old task, which are then interleaved with the new training data. This process forces the shared network representation to maintain its performance on the old tasks while accommodating the new one.65  
* **Modular and Compositional Approaches:** This is the most synergistic direction with the research proposed in this report. Instead of a single monolithic policy, knowledge is encapsulated in a library of modular skills. In robotics, for example, the Primitive Prompt Learning (PPL) framework first pre-trains a set of reusable "primitive prompts" that represent shared motion primitives. When a new skill needs to be learned, new task-specific prompts are learned and composed with the frozen, pre-existing primitive prompts. This architecture explicitly facilitates forward transfer by leveraging the shared primitives and inherently avoids catastrophic forgetting because the core knowledge base is not modified.69

The very architecture we are proposing—a MemoryManager that curates a library of discrete, composable skills—is naturally and powerfully suited to the lifelong learning paradigm. This architectural choice inherently sidesteps the primary mechanism of catastrophic forgetting that plagues monolithic neural network policies. In a standard deep RL agent, forgetting occurs because the weights of a single, large network are continuously updated to optimize for the most recent task, inevitably overwriting the weight configurations that encoded knowledge for previous tasks.64

A skill library architecture fundamentally changes this dynamic. Each skill can be stored as a distinct module—be it a separate policy network, a latent-conditioned policy within a larger model, or a set of descriptive prompts. When the agent encounters a new task and needs to learn a new skill, it can do so by creating a *new module* and simply adding it to the library. The parameters of the old, established skills can remain untouched and protected. The learning challenge is thus transformed. It is no longer a question of "how can we update this single giant network to do everything at once without breaking?" but rather "how do we learn a new skill module, and how do we update the high-level compositional policy to be aware of and utilize this new capability?" By adopting a skill-based procedural memory, we are not only improving generalization for the immediate task but are also laying the foundation for a system that is fundamentally more robust, scalable, and suited for the kind of open-ended, continual learning that the BLUEPRINT.md vision requires. This is a profound, second-order benefit of the proposed research.

## **Part IV: A Blueprint for Implementation and Future Research**

This final part of the report translates the extensive research and analysis into a concrete, actionable plan. It proposes a high-level system architecture for the new MemoryManager, recommends a phased implementation pathway, and identifies key open research problems that will shape the future of this domain.

### **Section 4.1: System Architecture for a Generative Procedural Memory**

To realize the vision of a generative skill library, the system's MemoryManager must be re-architected. The following components form the core of the proposed new system.

**Proposed Core Components:**

* **SkillLibrary:** This component is the heart of the new procedural memory, replacing the current list of monolithic action sequences. It will be a structured database or repository for storing learned skills. Each entry in the library will be a rich data structure representing a single skill, containing:  
  * **skill\_policy:** The executable policy itself. This could be a goal-conditioned policy π(a∣s,g), a latent-conditioned policy π(a∣s,z), or the weights of a dedicated neural network.  
  * **skill\_representation:** A compact vector embedding of the skill (e.g., derived from its policy parameters or its observed effects). This is crucial for efficient retrieval, allowing the system to find skills that are semantically or functionally similar to a given requirement.  
  * **skill\_metadata:** A collection of critical descriptive information. This should include a semantic description (potentially generated by an LLM), formal preconditions for initiation, learned termination conditions β, and a model of the skill's expected effects on the environment. This rich metadata transforms the library from a collection of black boxes into an interpretable and queryable knowledge base.  
* **SkillDiscoveryModule:** This is a dedicated offline or background process responsible for populating and curating the SkillLibrary. It would implement one of the advanced unsupervised learning algorithms discussed previously. A strong candidate for implementation is DUSDi, due to its focus on learning disentangled, composable skills.12 This module would run on large datasets of the agent's interaction with its environment (or a simulator), continuously discovering new fundamental behaviors and adding them to the library. This module could also incorporate an LLM-guided component, using a framework like L2S to generate skills directly from natural language descriptions or documentation, providing a powerful avenue for human-in-the-loop skill injection.23  
* **HierarchicalPolicyExecutor:** This is the online, real-time decision-making component of the system. It implements the HRL framework responsible for skill composition. A Feudal-style Manager/Worker architecture is highly recommended due to its demonstrated robustness and ability to learn meaningful, abstract goals.42 In this setup, the Manager would receive a high-level task, query the  
  SkillLibrary to select an appropriate skill or sequence of skills, and then issue abstract goals to the Worker. The Worker, a goal-conditioned policy, would then execute the retrieved skill\_policy to achieve the goal.

Architectural Flow:  
The proposed system would operate in a continuous loop of discovery and execution:

1. **Discovery (Offline/Background):** The SkillDiscoveryModule processes large batches of environment interaction data. It uses URL techniques to identify novel, reusable behaviors, encapsulates them as skill objects, and populates the SkillLibrary.  
2. **Task Reception (Online):** The agent receives a high-level task, either from an external user (e.g., in natural language) or from an internal planning module.  
3. **Decomposition and Planning:** A high-level reasoning layer, which could be the HRL Manager or an integrated LLM-planner, decomposes the task into a sequence of required sub-goals or semantic skill descriptions.  
4. **Skill Retrieval and Execution:** For each sub-goal in the plan, the HierarchicalPolicyExecutor queries the SkillLibrary to retrieve the most appropriate skill. The Manager then commands the Worker to execute this skill. This process repeats until the high-level task is complete.

Leveraging Open-Source Libraries:  
Building this system from scratch is unnecessary and inefficient. A robust implementation should leverage the mature ecosystem of open-source tools.

* **Core RL Framework (Ray RLlib):** RLlib is an industry-grade library for reinforcement learning that is exceptionally well-suited for this project. It is highly scalable, supporting distributed training across multiple machines and GPUs. Its key advantage is its composability; while it does not have a single, monolithic "HRL API," its flexible components (Algorithms, Policies, Sample Collectors) can be assembled to build custom HRL architectures, such as a Manager/Worker system.71 Its native support for multi-agent RL (MARL) provides a natural way to implement multi-level policies.71  
* **Simulation Environments (NVIDIA Isaac Lab, PyBullet):** For any tasks involving robotics or physics-based interaction, high-fidelity simulation is essential for safe and efficient training. NVIDIA Isaac Lab is a state-of-the-art, open-source framework built on Isaac Sim, providing photorealistic rendering and high-performance physics simulation optimized for robot learning.73 PyBullet is another widely used, lightweight, and fast physics simulator with extensive community support and many examples of HRL implementations.58  
* **Foundational Libraries:** The entire stack will be built upon standard deep learning libraries like PyTorch or TensorFlow, with Hugging Face's transformers library being the go-to for integrating LLMs for planning and semantic guidance.77

### **Section 4.2: Recommendations and Strategic Pathway**

To manage complexity and ensure steady progress, the evolution of the BLUEPRINT.md system should follow a phased approach, with each phase building upon the capabilities of the last.

* **Phase 1: Foundation \- Unsupervised Skill Discovery**  
  * **Action:** The immediate priority is to implement the SkillDiscoveryModule and the basic SkillLibrary. The initial implementation should focus on an unsupervised, MI-based skill discovery algorithm. A strong starting point would be a baseline like DIAYN, followed by an implementation of a DUSDi-like extension to produce a library of disentangled skills.12  
  * **Goal:** To move away from monolithic procedures by creating an initial, general-purpose SkillLibrary populated with low-level, reusable motor primitives or atomic tool-use actions. The success metric for this phase is the diversity and disentanglement of the learned skills.  
* **Phase 2: Composition \- Hierarchical Reinforcement Learning**  
  * **Action:** Implement the HierarchicalPolicyExecutor. A two-level HRL framework based on the Feudal paradigm is recommended for its robustness and the emergent properties of its goal-setting mechanism.42 The high-level Manager will be trained to select skills from the library created in Phase 1, and the low-level Worker will be a goal-conditioned policy trained to execute those skills.  
  * **Goal:** To enable the agent to solve novel, multi-step tasks by composing the foundational skills from its library. Success will be measured by the agent's ability to solve tasks that are compositionally harder than any single learned skill.  
* **Phase 3: Enhancement \- LLM-Guided Reasoning**  
  * **Action:** Integrate an LLM to serve as a high-level planner or "semantic front-end" for the HRL system. A framework like LDSC provides a clear blueprint, where the LLM receives a high-level command in natural language and outputs a sequence of semantic subgoals.24 These subgoals are then passed to the HRL Manager from Phase 2, which orchestrates their execution.  
  * **Goal:** To grant the agent the ability to understand and solve complex tasks specified in natural language, dramatically improving its flexibility, interpretability, and user-interactivity.  
* **Phase 4: Maturity \- Lifelong Skill Acquisition**  
  * **Action:** Evolve the architecture to support true lifelong learning. This involves implementing mechanisms to continually expand the SkillLibrary without catastrophic forgetting. This could be achieved by adopting a modular architecture where new skills are added as isolated modules (e.g., following the principles of PPL 69) or by using more advanced parameter-isolation techniques like learned masks.68 The system must be able to learn a new skill from a new task and seamlessly integrate it into its repertoire.  
  * **Goal:** To achieve a truly adaptive, lifelong learning agent that continuously improves its behavioral complexity and problem-solving capabilities over time, fulfilling the ultimate vision of the BLUEPRINT.md system.

### **Section 4.3: Open Research Problems and Future Directions**

While the proposed pathway is grounded in existing research, it is important to acknowledge that this is a frontier area of AI with significant open challenges. Progress will require not only engineering effort but also continued fundamental research.

* **Scalability:** While HRL is designed to mitigate the curse of dimensionality, scaling these systems to very deep hierarchies (more than two or three levels) or to solve extremely long-horizon tasks remains a formidable challenge.29 The computational cost, training instability, and difficulty of credit assignment in deep hierarchies are major open problems that limit the complexity of tasks we can currently solve.80  
* **Sample Efficiency and the Sim-to-Real Gap:** RL is notoriously sample-inefficient, often requiring millions or billions of interactions to learn a single task.80 In robotics, where real-world interaction is slow and costly, this is a primary bottleneck. Most complex skills will need to be learned in simulation, but transferring these skills to a physical robot—the "sim-to-real" problem—is a persistent challenge due to inevitable discrepancies between the simulated and real-world physics, sensing, and actuation.82  
* **Safety and Interpretability:** As agents begin to compose long and complex behaviors from a library of learned skills, ensuring that the resulting procedure is safe, reliable, and predictable becomes paramount. How can we provide formal safety guarantees on a policy composed of other neural network policies? How can we build systems that are interpretable, allowing a human user to understand *why* an agent chose a particular sequence of skills to solve a problem? These are critical questions for deploying such agents in high-stakes environments.57  
* **Automated Hierarchy Design:** Nearly all current HRL systems rely on a manually designed, fixed hierarchy (e.g., a two-level Manager/Worker structure). A grand challenge in the field is to develop methods that allow an agent to learn the hierarchical structure itself—autonomously determining the optimal number of layers and the appropriate level of temporal and state abstraction for each layer based on the task distribution.29  
* **Beyond Sequential Composition:** The current research focus is overwhelmingly on composing skills in a linear sequence. However, true intelligence and problem-solving often require more complex control structures, like a computer program. Future research must explore how to enable agents to compose skills in more sophisticated ways, such as parallel execution of multiple skills, conditional branching based on a skill's outcome, and looping constructs. This would elevate the skill library from a set of macros to a true, Turing-complete "programming language" of behaviors.59

Finally, it is worth considering the ultimate trajectory of this research. The current state-of-the-art focuses on discovering and composing a *fixed set* of learned skills. This is a crucial step, but the term "generative" in the user's query hints at a more profound capability. In other domains like image and text generation, generative AI excels at creating entirely novel artifacts by learning and operating within a continuous latent space.85 The next frontier for procedural memory is to apply this same principle to behavior. This would involve treating the

SkillLibrary not as a discrete collection of policies, but as a learned, continuous manifold in a high-dimensional "skill space." An agent could then generate entirely new, nuanced skills on the fly by interpolating between existing skills, extrapolating from them, or applying learned "skill modifier" operators within this latent space. This would transform the procedural memory from a library that is simply added to, into a true generative nexus of behavior—a system that can invent novel solutions rather than just composing existing ones. This represents the ultimate fulfillment of the research proposal's vision and a significant step towards more creative and adaptable artificial intelligence.

#### **Works cited**

1. Understanding AI Memory: A Deep Dive into the Cognitive Layers of Service Automation, accessed on June 16, 2025, [https://techsee.com/blog/understanding-ai-memory-a-deep-dive-into-the-cognitive-layers-of-service-automation/](https://techsee.com/blog/understanding-ai-memory-a-deep-dive-into-the-cognitive-layers-of-service-automation/)  
2. What Is AI Agent Memory? | IBM, accessed on June 16, 2025, [https://www.ibm.com/think/topics/ai-agent-memory](https://www.ibm.com/think/topics/ai-agent-memory)  
3. Why Memory Matters for AI Agents: Insights from Nikolay Penkov \- Arya.ai, accessed on June 16, 2025, [https://arya.ai/blog/why-memory-matters-for-ai-agents-insights-from-nikolay-penkov](https://arya.ai/blog/why-memory-matters-for-ai-agents-insights-from-nikolay-penkov)  
4. Procedural Knowledge | SC Training (formerly EdApp) Microlearning, accessed on June 16, 2025, [https://training.safetyculture.com/blog/procedural-knowledge/](https://training.safetyculture.com/blog/procedural-knowledge/)  
5. Procedural knowledge \- Wikipedia, accessed on June 16, 2025, [https://en.wikipedia.org/wiki/Procedural\_knowledge](https://en.wikipedia.org/wiki/Procedural_knowledge)  
6. Reinforcement learning \- Wikipedia, accessed on June 16, 2025, [https://en.wikipedia.org/wiki/Reinforcement\_learning](https://en.wikipedia.org/wiki/Reinforcement_learning)  
7. \#9: Does AI Remember? The Role of Memory in Agentic Workflows \- Hugging Face, accessed on June 16, 2025, [https://huggingface.co/blog/Kseniase/memory](https://huggingface.co/blog/Kseniase/memory)  
8. Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents, accessed on June 16, 2025, [https://arxiv.org/abs/2505.03434](https://arxiv.org/abs/2505.03434)  
9. Generalization to New Actions in Reinforcement Learning, accessed on June 16, 2025, [http://proceedings.mlr.press/v119/jain20b/jain20b.pdf](http://proceedings.mlr.press/v119/jain20b/jain20b.pdf)  
10. On the Power of Pre-training for Generalization in RL:Provable ..., accessed on June 16, 2025, [https://proceedings.mlr.press/v202/ye23a/ye23a.pdf](https://proceedings.mlr.press/v202/ye23a/ye23a.pdf)  
11. Procedural Knowledge \- Types claude \- follow the idea \- Obsidian Publish, accessed on June 16, 2025, [https://publish.obsidian.md/followtheidea/Content/AI/Procedural+Knowledge+-+Types+++++claude](https://publish.obsidian.md/followtheidea/Content/AI/Procedural+Knowledge+-+Types+++++claude)  
12. arxiv.org, accessed on June 16, 2025, [https://arxiv.org/html/2410.11251v1](https://arxiv.org/html/2410.11251v1)  
13. Unsupervised Skill Discovery in Reinforcement Learning (RL) \- Emergent Mind, accessed on June 16, 2025, [https://www.emergentmind.com/topics/unsupervised-skill-discovery-method](https://www.emergentmind.com/topics/unsupervised-skill-discovery-method)  
14. METRA: Scalable Unsupervised RL with Metric-Aware Abstraction \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2310.08887v2](https://arxiv.org/html/2310.08887v2)  
15. Hierarchical Multi-Agent Skill Discovery, accessed on June 16, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2023/file/c276c3303c0723c83a43b95a44a1fcbf-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/c276c3303c0723c83a43b95a44a1fcbf-Paper-Conference.pdf)  
16. Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning \- UT Computer Science, accessed on June 16, 2025, [https://www.cs.utexas.edu/\~pstone/Papers/bib2html-links/hu\_neurips2024.pdf](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/hu_neurips2024.pdf)  
17. (PDF) Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning \- ResearchGate, accessed on June 16, 2025, [https://www.researchgate.net/publication/384938663\_Disentangled\_Unsupervised\_Skill\_Discovery\_for\_Efficient\_Hierarchical\_Reinforcement\_Learning](https://www.researchgate.net/publication/384938663_Disentangled_Unsupervised_Skill_Discovery_for_Efficient_Hierarchical_Reinforcement_Learning)  
18. Wasserstein Unsupervised Reinforcement Learning, accessed on June 16, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/20645/20404](https://ojs.aaai.org/index.php/AAAI/article/view/20645/20404)  
19. Task Adaptation from Skills: Information Geometry ... \- arXiv, accessed on June 16, 2025, [https://arxiv.org/pdf/2506.10629](https://arxiv.org/pdf/2506.10629)  
20. TASK ADAPTATION FROM SKILLS: INFORMATION GE- OMETRY, DISENTANGLEMENT, AND NEW OBJECTIVES FOR UNSUPERVISED REINFORCEMENT LEARNING \- OpenReview, accessed on June 16, 2025, [https://openreview.net/notes/edits/attachment?id=DZoo5W1jwD\&name=pdf](https://openreview.net/notes/edits/attachment?id=DZoo5W1jwD&name=pdf)  
21. \[2405.16030\] Constrained Ensemble Exploration for Unsupervised Skill Discovery \- arXiv, accessed on June 16, 2025, [https://arxiv.org/abs/2405.16030](https://arxiv.org/abs/2405.16030)  
22. Language Guided Skill Discovery \- PopAi, accessed on June 16, 2025, [https://www.popai.pro/chatwithdoc/paper/Language-Guided-Skill-Discovery.html?ref=gblog.popai.pro](https://www.popai.pro/chatwithdoc/paper/Language-Guided-Skill-Discovery.html?ref=gblog.popai.pro)  
23. Skill Discovery using Language Models \- OpenReview, accessed on June 16, 2025, [https://openreview.net/forum?id=DBbgasVgyQ](https://openreview.net/forum?id=DBbgasVgyQ)  
24. arxiv.org, accessed on June 16, 2025, [https://arxiv.org/html/2503.19007v1](https://arxiv.org/html/2503.19007v1)  
25. Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning, accessed on June 16, 2025, [https://www.researchgate.net/publication/390177314\_Option\_Discovery\_Using\_LLM-guided\_Semantic\_Hierarchical\_Reinforcement\_Learning?\_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19](https://www.researchgate.net/publication/390177314_Option_Discovery_Using_LLM-guided_Semantic_Hierarchical_Reinforcement_Learning?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19)  
26. Language Guided Skill Discovery | OpenReview, accessed on June 16, 2025, [https://openreview.net/forum?id=i3e92uSZCp](https://openreview.net/forum?id=i3e92uSZCp)  
27. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2504.03160v1](https://arxiv.org/html/2504.03160v1)  
28. CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills using Large Language Models \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2409.18382v2](https://arxiv.org/html/2409.18382v2)  
29. Hierarchical Reinforcement Learning, Sequential Behavior, and the Dorsal Frontostriatal System \- PMC, accessed on June 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9274316/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9274316/)  
30. The Promise of Hierarchical Reinforcement Learning \- The Gradient, accessed on June 16, 2025, [https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/)  
31. Hierarchical Reinforcement Learning (HRL) in AI \- GeeksforGeeks, accessed on June 16, 2025, [https://www.geeksforgeeks.org/artificial-intelligence/hierarchical-reinforcement-learning-hrl-in-ai/](https://www.geeksforgeeks.org/artificial-intelligence/hierarchical-reinforcement-learning-hrl-in-ai/)  
32. Hierarchical Reinforcement Learning: A Comprehensive Overview \- MarkTechPost, accessed on June 16, 2025, [https://www.marktechpost.com/2024/05/20/hierarchical-reinforcement-learning-a-comprehensive-overview/](https://www.marktechpost.com/2024/05/20/hierarchical-reinforcement-learning-a-comprehensive-overview/)  
33. Hierarchical Reinforcement Learning: A Survey and Open Research Challenges \- MDPI, accessed on June 16, 2025, [https://www.mdpi.com/2504-4990/4/1/9](https://www.mdpi.com/2504-4990/4/1/9)  
34. Does Hierarchical Reinforcement Learning Outperform Standard Reinforcement Learning in Goal-Oriented Sparse Reward Domains? \- OpenReview, accessed on June 16, 2025, [https://openreview.net/pdf?id=DrosGMxl2v](https://openreview.net/pdf?id=DrosGMxl2v)  
35. Does Hierarchial Reinforcement Learning work yet? – Sholto's Blog ..., accessed on June 16, 2025, [https://sholtodouglas.github.io/DoesHierarchialRLWorkYet/](https://sholtodouglas.github.io/DoesHierarchialRLWorkYet/)  
36. MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2402.14244v2](https://arxiv.org/html/2402.14244v2)  
37. Successor Options: An Option Discovery Framework for Reinforcement Learning \- IJCAI, accessed on June 16, 2025, [https://www.ijcai.org/proceedings/2019/0458.pdf](https://www.ijcai.org/proceedings/2019/0458.pdf)  
38. Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining \- Duke Computer Science, accessed on June 16, 2025, [https://users.cs.duke.edu/\~gdk/pubs/skillchain.pdf](https://users.cs.duke.edu/~gdk/pubs/skillchain.pdf)  
39. Modular Hierarchical Reinforcement Learning for Robotics: Improving Scalability and Generalizability \- OpenReview, accessed on June 16, 2025, [https://openreview.net/pdf?id=KIF8sUbr2k](https://openreview.net/pdf?id=KIF8sUbr2k)  
40. Learning Options in Reinforcement Learning, accessed on June 16, 2025, [https://neuro.bstu.by/ai/Win-old/Conf-4/Icnnai-2008/Submission/Survey/stolle2002learning.pdf](https://neuro.bstu.by/ai/Win-old/Conf-4/Icnnai-2008/Submission/Survey/stolle2002learning.pdf)  
41. \[1905.05731\] Successor Options: An Option Discovery Framework for Reinforcement Learning \- arXiv, accessed on June 16, 2025, [https://arxiv.org/abs/1905.05731](https://arxiv.org/abs/1905.05731)  
42. FeUdal Networks for Hierarchical Reinforcement Learning, accessed on June 16, 2025, [https://proceedings.mlr.press/v70/vezhnevets17a/vezhnevets17a.pdf](https://proceedings.mlr.press/v70/vezhnevets17a/vezhnevets17a.pdf)  
43. Option Discovery using Deep Skill Chaining \- Brown CS, accessed on June 16, 2025, [https://cs.brown.edu/people/gdk/pubs/dsc\_deeprl\_ws.pdf](https://cs.brown.edu/people/gdk/pubs/dsc_deeprl_ws.pdf)  
44. Feudal Reinforcement Learning \- Computer Science, accessed on June 16, 2025, [https://www.cs.toronto.edu/\~fritz/absps/dh93.pdf](https://www.cs.toronto.edu/~fritz/absps/dh93.pdf)  
45. FeUdal Networks for Hierarchical Reinforcement Learning \- Minkyu Choi's Personal Webpage, accessed on June 16, 2025, [https://minkyuchoi-07.github.io/2023/03/20/feudal/](https://minkyuchoi-07.github.io/2023/03/20/feudal/)  
46. Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning, accessed on June 16, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2023/file/c5ed2c8acda8c3716b1b6f9c6c713aaa-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/c5ed2c8acda8c3716b1b6f9c6c713aaa-Paper-Conference.pdf)  
47. A tale of two goals: leveraging sequentiality in multi-goal scenarios \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2503.21677v1](https://arxiv.org/html/2503.21677v1)  
48. Goal-conditioned Reinforcement Learning with Subgoals Generated from Relabeling, accessed on June 16, 2025, [https://openreview.net/forum?id=m7lBCyROPP](https://openreview.net/forum?id=m7lBCyROPP)  
49. Goal-Conditioned On-Policy Reinforcement Learning \- NIPS, accessed on June 16, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2024/file/51c6e143b5da2bd6e4a618d8a5d7f38b-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/51c6e143b5da2bd6e4a618d8a5d7f38b-Paper-Conference.pdf)  
50. Goal-Conditioned On-Policy Reinforcement Learning \- OpenReview, accessed on June 16, 2025, [https://openreview.net/forum?id=KP7EUORJYI¬eId=pVKz2Y4FRJ](https://openreview.net/forum?id=KP7EUORJYI&noteId=pVKz2Y4FRJ)  
51. \[2201.08299\] Goal-Conditioned Reinforcement Learning: Problems and Solutions \- arXiv, accessed on June 16, 2025, [https://arxiv.org/abs/2201.08299](https://arxiv.org/abs/2201.08299)  
52. Hierarchical Reinforcement Learning for Playing a Dynamic Dungeon Crawler Game, accessed on June 16, 2025, [https://www.researchgate.net/publication/331153031\_Hierarchical\_Reinforcement\_Learning\_for\_Playing\_a\_Dynamic\_Dungeon\_Crawler\_Game](https://www.researchgate.net/publication/331153031_Hierarchical_Reinforcement_Learning_for_Playing_a_Dynamic_Dungeon_Crawler_Game)  
53. \[1703.01161\] FeUdal Networks for Hierarchical Reinforcement Learning \- arXiv, accessed on June 16, 2025, [https://arxiv.org/abs/1703.01161](https://arxiv.org/abs/1703.01161)  
54. (PDF) Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining., accessed on June 16, 2025, [https://www.researchgate.net/publication/221620630\_Skill\_Discovery\_in\_Continuous\_Reinforcement\_Learning\_Domains\_using\_Skill\_Chaining](https://www.researchgate.net/publication/221620630_Skill_Discovery_in_Continuous_Reinforcement_Learning_Domains_using_Skill_Chaining)  
55. irl.cs.brown.edu, accessed on June 16, 2025, [http://irl.cs.brown.edu/pubs/skillchain-tech.pdf](http://irl.cs.brown.edu/pubs/skillchain-tech.pdf)  
56. Skill chaining \- Wikipedia, accessed on June 16, 2025, [https://en.wikipedia.org/wiki/Skill\_chaining](https://en.wikipedia.org/wiki/Skill_chaining)  
57. Hierarchical Reinforcement Learning Integrating With Human Knowledge for Practical Robot Skill Learning in Complex Multi-Stage Manipulation | Request PDF \- ResearchGate, accessed on June 16, 2025, [https://www.researchgate.net/publication/372234229\_Hierarchical\_Reinforcement\_Learning\_Integrating\_with\_Human\_Knowledge\_for\_Practical\_Robot\_Skill\_Learning\_in\_Complex\_Multi-Stage\_Manipulation](https://www.researchgate.net/publication/372234229_Hierarchical_Reinforcement_Learning_Integrating_with_Human_Knowledge_for_Practical_Robot_Skill_Learning_in_Complex_Multi-Stage_Manipulation)  
58. Hierarchical Reinforcement Learning Based on Planning Operators\* \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2309.14237v2](https://arxiv.org/html/2309.14237v2)  
59. DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2505.00527v1](https://arxiv.org/html/2505.00527v1)  
60. On the Importance of Exploration for Generalization in Reinforcement Learning, accessed on June 16, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2023/file/2a4310c4fd24bd336aa2f64f93cb5d39-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/2a4310c4fd24bd336aa2f64f93cb5d39-Paper-Conference.pdf)  
61. Generalisation in Reinforcement Learning | Robert Kirk, accessed on June 16, 2025, [https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html)  
62. Quantifying generalization in reinforcement learning \- OpenAI, accessed on June 16, 2025, [https://openai.com/index/quantifying-generalization-in-reinforcement-learning/](https://openai.com/index/quantifying-generalization-in-reinforcement-learning/)  
63. Generalization Rules in AI \- GeeksforGeeks, accessed on June 16, 2025, [https://www.geeksforgeeks.org/artificial-intelligence/generalization-rules-in-ai/](https://www.geeksforgeeks.org/artificial-intelligence/generalization-rules-in-ai/)  
64. Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models \- arXiv, accessed on June 16, 2025, [https://arxiv.org/html/2503.12923v1](https://arxiv.org/html/2503.12923v1)  
65. Lifelong Machine Learning Systems: Beyond Learning Algorithms, accessed on June 16, 2025, [https://axon.cs.byu.edu/\~martinez/classes/678/Presentations/Martin.pdf](https://axon.cs.byu.edu/~martinez/classes/678/Presentations/Martin.pdf)  
66. Policy and Value Transfer in Lifelong Reinforcement ... \- Brown CS, accessed on June 16, 2025, [https://cs.brown.edu/people/gdk/pubs/pandv\_transfer.pdf](https://cs.brown.edu/people/gdk/pubs/pandv_transfer.pdf)  
67. Advancing autonomy through lifelong learning: a survey of autonomous intelligent systems, accessed on June 16, 2025, [https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1385778/full](https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1385778/full)  
68. Lifelong Reinforcement Learning with Modulating Masks \- OpenReview, accessed on June 16, 2025, [https://openreview.net/forum?id=V7tahqGrOq](https://openreview.net/forum?id=V7tahqGrOq)  
69. Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation \- CVF Open Access, accessed on June 16, 2025, [https://openaccess.thecvf.com/content/CVPR2025/papers/Yao\_Think\_Small\_Act\_Big\_Primitive\_Prompt\_Learning\_for\_Lifelong\_Robot\_CVPR\_2025\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Think_Small_Act_Big_Primitive_Prompt_Learning_for_Lifelong_Robot_CVPR_2025_paper.pdf)  
70. Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot ..., accessed on June 16, 2025, [https://arxiv.org/pdf/2504.00420](https://arxiv.org/pdf/2504.00420)  
71. RLlib: Industry-Grade, Scalable Reinforcement Learning — Ray 2.47.0, accessed on June 16, 2025, [https://docs.ray.io/en/latest/rllib/index.html](https://docs.ray.io/en/latest/rllib/index.html)  
72. Introducing RLlib: A composable and scalable reinforcement learning library – O'Reilly, accessed on June 16, 2025, [https://www.oreilly.com/content/introducing-rllib-a-composable-and-scalable-reinforcement-learning-library/](https://www.oreilly.com/content/introducing-rllib-a-composable-and-scalable-reinforcement-learning-library/)  
73. NVIDIA Isaac Lab, accessed on June 16, 2025, [https://developer.nvidia.com/isaac/lab](https://developer.nvidia.com/isaac/lab)  
74. Isaac Lab — Isaac Sim Documentation, accessed on June 16, 2025, [https://docs.isaacsim.omniverse.nvidia.com/4.5.0/isaac\_lab\_tutorials/index.html](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/isaac_lab_tutorials/index.html)  
75. Learning to Walk Using Hierarchical Reinforcement Learning, accessed on June 16, 2025, [https://www.cs.cmu.edu/\~epxing/Class/10708-19/assets/project/final-reports/project4.pdf](https://www.cs.cmu.edu/~epxing/Class/10708-19/assets/project/final-reports/project4.pdf)  
76. Hierarchical Model-Based Reinforcement Learning with Temporal Abstractions | Open Research Commons, accessed on June 16, 2025, [https://bcommons.berkeley.edu/hierarchical-model-based-reinforcement-learning-temporal-abstractions](https://bcommons.berkeley.edu/hierarchical-model-based-reinforcement-learning-temporal-abstractions)  
77. Top 7 Open-Source Robotics Platforms \- ThinkRobotics.com, accessed on June 16, 2025, [https://thinkrobotics.com/blogs/learn/top-7-open-source-robotics-platforms](https://thinkrobotics.com/blogs/learn/top-7-open-source-robotics-platforms)  
78. Tools | DeepRob: Deep Learning for Robot Perception, accessed on June 16, 2025, [https://deeprob.org/w25/tools/](https://deeprob.org/w25/tools/)  
79. addy1997/Robotics-Resources: List of commonly used robotics libraries and packages \- GitHub, accessed on June 16, 2025, [https://github.com/addy1997/Robotics-Resources](https://github.com/addy1997/Robotics-Resources)  
80. What are the challenges with scaling reinforcement learning models?, accessed on June 16, 2025, [https://milvus.io/ai-quick-reference/what-are-the-challenges-with-scaling-reinforcement-learning-models](https://milvus.io/ai-quick-reference/what-are-the-challenges-with-scaling-reinforcement-learning-models)  
81. Mind the GAP\! The Challenges of Scale in Pixel-based Deep Reinforcement Learning, accessed on June 16, 2025, [https://arxiv.org/html/2505.17749v1](https://arxiv.org/html/2505.17749v1)  
82. Deep Reinforcement Learning: A Chronological Overview and Methods \- MDPI, accessed on June 16, 2025, [https://www.mdpi.com/2673-2688/6/3/46](https://www.mdpi.com/2673-2688/6/3/46)  
83. Hierarchical Reinforcement Learning for Quadrupedal Robots: Efficient Object Manipulation in Constrained Environments \- PMC \- PubMed Central, accessed on June 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11902496/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11902496/)  
84. Composition and Incremental Refinement of Skill Models For Robotic Assembly Tasks \- Fraunhofer-Publica, accessed on June 16, 2025, [https://publica.fraunhofer.de/bitstreams/ab7c26ee-4705-4f90-921d-1bdeeab54fd6/download](https://publica.fraunhofer.de/bitstreams/ab7c26ee-4705-4f90-921d-1bdeeab54fd6/download)  
85. Generative AI use cases for the enterprise \- IBM, accessed on June 16, 2025, [https://www.ibm.com/think/topics/generative-ai-use-cases](https://www.ibm.com/think/topics/generative-ai-use-cases)  
86. Generative AI in Libraries: Use Cases in Instruction, Evidence Synthesis, Research... \- Janice Kung \- YouTube, accessed on June 16, 2025, [https://www.youtube.com/watch?v=\_agktZaUgqo](https://www.youtube.com/watch?v=_agktZaUgqo)
